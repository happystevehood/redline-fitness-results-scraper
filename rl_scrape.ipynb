{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redline Results Scraper\n",
    "\n",
    "This notebook scrapes Redlines Fitness games results data from multiple events hosted on the runnersunite.racetecresults.com website. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup, Define URLs Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Ready to scrape 4 Days of Events.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import lxml\n",
    "from io import StringIO\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import pprint\n",
    "import os\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# Dictionary of events with their names and result URLs\n",
    "day_urls = {\n",
    "        #future proofing for 2025\n",
    "    #\"2025KL.1\": \"\",\n",
    "    #\"2025KL.2\": \"\",\n",
    "    \n",
    "    \"2024.1\":   \"https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251\",\n",
    "    \"2024.2\":   \"https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252\",    \n",
    "    \"2023.1\":   \"https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1216\",\n",
    "    \"2023.2\":   \"https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217\",\n",
    "}\n",
    "\n",
    "\n",
    "EVENT_RENAME_LIST = [\n",
    "    # 2025\n",
    "    #[\"WomensSinglesAdvKL2025\",   \"2025KL.1 Women's Singles Level 3\"],\n",
    "    #[\"MensSinglesAdvKL2025\",     \"2025KL.1 Men's Singles Level 3\",\n",
    "    #[\"WomensSinglesInterKL2025\", \"2025KL.1 Women's Singles Level 2\"],\n",
    "    #[\"MensSinglesInterKL2025\",   \"2025KL.2 Men's Singles Level 2\"],\n",
    "    #[\"WomensDoublesKL2025\",      \"2025KL.2 Women's Doubles\"],\n",
    "    #[\"MensDoublesKL2025\",        \"2025KL.1 Men's Doubles\"],\n",
    "    #[\"MixedDoublesKL2025\",       \"2025KL.1 Mixed Doubles\"],\n",
    "    #[\"TeamRelayWomenKL2025\",     \"2025KL.2 Team Relay Women\"],\n",
    "    #[\"TeamRelayMenKL2025\",       \"2025KL.2 Team Relay Men\"],\n",
    "    #[\"TeamRelayMixedKL2025\",     \"2025KL.2 Team Relay Mixed\"],\n",
    "    #[\"BeginnersKL2025\",          \"2025KL.1 Singles Level 1\"],\n",
    "    \n",
    "    #2024\n",
    "    [\"WomensSinglesCompetitive2024\", \"2024.1 Women's Singles Competitive\"],\n",
    "    [\"MensSinglesCompetitive2024\"  , \"2024.1 Men's Singles Competitive\"],\n",
    "    [\"WomensSinglesOpen2024\",        \"2024.1 Women's Singles Open\"],\n",
    "    [\"MensSinglesOpen2024\",          \"2024.2 Men's Singles Open\"],\n",
    "    [\"WomensDoubles2024\",            \"2024.2 Women's Doubles\"],\n",
    "    [\"MensDoubles2024\",              \"2024.1 Men's Doubles\"],\n",
    "    [\"MixedDoubles2024\",             \"2024.1 Mixed Doubles\"],\n",
    "    [\"TeamRelayWomen2024\",           \"2024.2 Team Relay Women\"],\n",
    "    [\"TeamRelayMen2024\",             \"2024.2 Team Relay Men\"],\n",
    "    [\"TeamRelayMixed2024\",           \"2024.2 Team Relay Mixed\"],\n",
    "    [\"Beginners2024\",                \"2024.1 Beginners\"],\n",
    "    \n",
    "    #2023\n",
    "    [\"WomensSinglesCompetitive2023\", \"2023.1 Women's Singles Competitive\"],\n",
    "    [\"MensSinglesCompetitive2023\",   \"2023.1 Men's Singles Competitive\"],\n",
    "    [\"WomensSinglesOpen2023\",        \"2023.1 Women's Singles Open\"],\n",
    "    [\"MensSinglesOpen2023\",          \"2023.1 Men's Singles Open\"],\n",
    "    [\"WomensDoubles2023\",            \"2023.2 Women's Doubles\"],\n",
    "    [\"MensDoubles2023\",              \"2023.2 Men's Doubles\"],\n",
    "    [\"MixedDoubles2023\",             \"2023.2 Mixed Doubles\"],\n",
    "    [\"TeamRelayWomen2023\",           \"2023.2 Team Relay Women\"],\n",
    "    [\"TeamRelayMen2023\",             \"2023.2 Team Relay Men\"],\n",
    "    [\"TeamRelayMixed2023\",           \"2023.2 Team Relay Mixed\"],\n",
    "]\n",
    "\n",
    "# helper fucntion to get the total number of pages for a given event\n",
    "def get_total_pages(html_content: str) -> int:\n",
    "    \"\"\"\n",
    "    Parses HTML content to find the total number of result pages.\n",
    "\n",
    "    Args:\n",
    "        html_content: The HTML source of the results page.\n",
    "\n",
    "    Returns:\n",
    "        The total number of pages as an integer.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "    # 1. Find the pager table by its unique ID\n",
    "    pager_table = soup.find('table', id='ctl00_Content_Main_grdTopPager')\n",
    "\n",
    "    # 2. If no pager table is found, there's only one page of results\n",
    "    if not pager_table:\n",
    "        return 1\n",
    "\n",
    "    # 3. Find all the link tags within the pager table\n",
    "    page_links = pager_table.find_all('a')\n",
    "    \n",
    "    page_numbers = []\n",
    "    # 4. Loop through the links, clean the text, and convert to a number\n",
    "    for link in page_links:\n",
    "        # Get text like \"[1]\" or \"2\" and remove whitespace\n",
    "        text = link.get_text(strip=True)\n",
    "        \n",
    "        # Remove decorative brackets\n",
    "        cleaned_text = text.replace('[', '').replace(']', '')\n",
    "        \n",
    "        # Check if the result is a digit before converting\n",
    "        if cleaned_text.isdigit():\n",
    "            page_numbers.append(int(cleaned_text))\n",
    "\n",
    "    # 5. The total number of pages is the highest number found.\n",
    "    # If for some reason no numbers were found, default to 1.\n",
    "    if not page_numbers:\n",
    "        return 1\n",
    "    else:\n",
    "        return max(page_numbers)\n",
    "\n",
    "#helper function to remove the second duplicate column name\n",
    "def remove_second_duplicate_column(dataframe, col_name_to_check):\n",
    "    \"\"\"\n",
    "    Checks for a duplicate column by name and removes the second occurrence.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input DataFrame.\n",
    "        col_name_to_check (str): The name of the column to check for duplicates.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the duplicate column removed, \n",
    "                      or the original DataFrame if no duplicates were found.\n",
    "    \"\"\"\n",
    "    # Find the integer positions of all columns with the given name\n",
    "    indices = [i for i, name in enumerate(dataframe.columns) if name == col_name_to_check]\n",
    "\n",
    "    if len(indices) > 1:\n",
    "        #print(f\"Found duplicate '{col_name_to_check}' columns at positions: {indices}\")\n",
    "        \n",
    "        # Get the position of the second occurrence\n",
    "        col_to_drop_index = indices[1]\n",
    "        #print(f\"Removing the second occurrence at position {col_to_drop_index}.\")\n",
    "        \n",
    "        # Get a list of all column indices except the one to drop\n",
    "        cols_to_keep_indices = [i for i in range(len(dataframe.columns)) if i != col_to_drop_index]\n",
    "        \n",
    "        # Return a new DataFrame with only the desired columns\n",
    "        return dataframe.iloc[:, cols_to_keep_indices]\n",
    "    else:\n",
    "        print(f\"No duplicates found for column '{col_name_to_check}'.\")\n",
    "        #print(f\"Columns: '{dataframe.iloc[0]}'\")\n",
    "        return dataframe.copy() # Return a copy to avoid side effects\n",
    "\n",
    "def polite_delay(min_seconds=10, max_seconds=45):\n",
    "    wait_time = random.uniform(min_seconds, max_seconds)\n",
    "    print(f\"Waiting for {wait_time:.2f} seconds...\")\n",
    "    time.sleep(wait_time)\n",
    "\n",
    "print(f\"Ready to scrape {len(day_urls)} Days of Events.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The First Scraping Loop (to get the event links)\n",
    "Needed to use Selenium to avoid website blocking access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for: '2024.1'...\n",
      "Navigating to URL...\n",
      "Scraping data for: '2024.2'...\n",
      "Navigating to URL...\n",
      "Scraping data for: '2023.1'...\n",
      "Navigating to URL...\n",
      "Scraping data for: '2023.2'...\n",
      "Navigating to URL...\n",
      "[{'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=1',\n",
      "  'text': \"2024.1 Women's Singles Open\"},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=2',\n",
      "  'text': \"2024.1 Women's Singles Competitive\"},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=4',\n",
      "  'text': \"2024.1 Men's Singles Competitive\"},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=6',\n",
      "  'text': '2024.1 Mixed Doubles'},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=5',\n",
      "  'text': \"2024.1 Men's Doubles\"},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=7',\n",
      "  'text': '2024.1 Beginners'},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=1',\n",
      "  'text': \"2024.2 Men's Singles Open\"},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=2',\n",
      "  'text': \"2024.2 Women's Doubles\"},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=5',\n",
      "  'text': '2024.2 Team Relay Men'},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=6',\n",
      "  'text': '2024.2 Team Relay Mixed'},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=4',\n",
      "  'text': '2024.2 Team Relay Women'},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1216&EId=1',\n",
      "  'text': \"2023.1 Women's Singles Open\"},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1216&EId=2',\n",
      "  'text': \"2023.1 Women's Singles Competitive\"},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1216&EId=3',\n",
      "  'text': \"2023.1 Men's Singles Open\"},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1216&EId=4',\n",
      "  'text': \"2023.1 Men's Singles Competitive\"},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=1',\n",
      "  'text': \"2023.2 Women's Doubles\"},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=2',\n",
      "  'text': \"2023.2 Men's Doubles\"},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=3',\n",
      "  'text': '2023.2 Mixed Doubles'},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=4',\n",
      "  'text': '2023.2 Team Relay Women'},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=5',\n",
      "  'text': '2023.2 Team Relay Men'},\n",
      " {'href': 'https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=6',\n",
      "  'text': '2023.2 Team Relay Mixed'}]\n",
      "\n",
      "Scraping process complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Extracted data across all events.\n",
    "extracted_data = []\n",
    "\n",
    "# Loop through the dictionary of URLs\n",
    "for year_day, url in day_urls.items():\n",
    "    print(f\"Scraping data for: '{year_day}'...\")\n",
    "    \n",
    "    try:\n",
    "\n",
    "        # Setup and run a real Chrome browser\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "        print(\"Navigating to URL...\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the page (and any javascript) to load\n",
    "        time.sleep(3) \n",
    "\n",
    "        # Get the HTML content after the browser has rendered it\n",
    "        html_content = driver.page_source\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "        # 1. Create a BeautifulSoup object to parse the HTML\n",
    "        # We use the 'lxml' parser, which is fast and reliable.\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "        # 2. Find the <ul> element by its unique ID\n",
    "        # This is the most reliable way to select the container.\n",
    "        event_list = soup.find('ul', id='ctl00_Content_Main_divEvents')\n",
    "\n",
    "        # 3. From that container, find all the <a> (anchor/link) tags\n",
    "        all_links = event_list.find_all('a')\n",
    "\n",
    "        # 4. Loop through the links and extract the data\n",
    "        # We'll store the results in a list of dictionaries for clarity.\n",
    "        base_url = \"https://runnersunite.racetecresults.com\"\n",
    "\n",
    "        for link in all_links:\n",
    "            # Get the value of the 'href' attribute.\n",
    "            # Note: BeautifulSoup automatically converts '&' to '&' for you.\n",
    "            relative_href = link.get('href')\n",
    "            \n",
    "            # Get the visible text of the link, removing any extra whitespace\n",
    "            link_text = link.get_text(strip=True)\n",
    "            \n",
    "            # Create the full, usable URL\n",
    "            full_url = f\"{base_url}/{relative_href}\"\n",
    "            full_text = f\"{year_day} {link_text}\"\n",
    "            \n",
    "            extracted_data.append({\n",
    "                'text': full_text,\n",
    "                'href': full_url\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"--> An error occurred while scraping {url}: {e}\")\n",
    "\n",
    "    # Be a good web citizen and add a small delay between requests\n",
    "    time.sleep(2)\n",
    "\n",
    "# Print the results in a clean format\n",
    "pprint.pprint(extracted_data)\n",
    "\n",
    "print(\"\\nScraping process complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: The 2nd Scraping Loop (to get all the data and save files)\n",
    "Added random delay at end of every file so website dont get mad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for: '2024.1 Women's Singles Open'...\n",
      "Navigating to URL...\n",
      "Found 5 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=1&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=1&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "  > Scraping page 3: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=1&dt=0&PageNo=3\n",
      "Navigating to page URL...\n",
      "  > Scraping page 4: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=1&dt=0&PageNo=4\n",
      "Navigating to page URL...\n",
      "  > Scraping page 5: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=1&dt=0&PageNo=5\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2024.1 Women's Singles Open.csv:\n",
      "Waiting for 21.68 seconds...\n",
      "Scraping data for: '2024.1 Women's Singles Competitive'...\n",
      "Navigating to URL...\n",
      "Found 2 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=2&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=2&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2024.1 Women's Singles Competitive.csv:\n",
      "Waiting for 37.53 seconds...\n",
      "Scraping data for: '2024.1 Men's Singles Competitive'...\n",
      "Navigating to URL...\n",
      "Found 5 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=4&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=4&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "  > Scraping page 3: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=4&dt=0&PageNo=3\n",
      "Navigating to page URL...\n",
      "  > Scraping page 4: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=4&dt=0&PageNo=4\n",
      "Navigating to page URL...\n",
      "  > Scraping page 5: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=4&dt=0&PageNo=5\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2024.1 Men's Singles Competitive.csv:\n",
      "Waiting for 30.48 seconds...\n",
      "Scraping data for: '2024.1 Mixed Doubles'...\n",
      "Navigating to URL...\n",
      "Found 9 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=6&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=6&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "  > Scraping page 3: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=6&dt=0&PageNo=3\n",
      "Navigating to page URL...\n",
      "  > Scraping page 4: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=6&dt=0&PageNo=4\n",
      "Navigating to page URL...\n",
      "  > Scraping page 5: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=6&dt=0&PageNo=5\n",
      "Navigating to page URL...\n",
      "  > Scraping page 6: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=6&dt=0&PageNo=6\n",
      "Navigating to page URL...\n",
      "  > Scraping page 7: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=6&dt=0&PageNo=7\n",
      "Navigating to page URL...\n",
      "  > Scraping page 8: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=6&dt=0&PageNo=8\n",
      "Navigating to page URL...\n",
      "  > Scraping page 9: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=6&dt=0&PageNo=9\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2024.1 Mixed Doubles.csv:\n",
      "Waiting for 35.54 seconds...\n",
      "Scraping data for: '2024.1 Men's Doubles'...\n",
      "Navigating to URL...\n",
      "Found 7 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=5&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=5&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "  > Scraping page 3: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=5&dt=0&PageNo=3\n",
      "Navigating to page URL...\n",
      "  > Scraping page 4: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=5&dt=0&PageNo=4\n",
      "Navigating to page URL...\n",
      "  > Scraping page 5: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=5&dt=0&PageNo=5\n",
      "Navigating to page URL...\n",
      "  > Scraping page 6: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=5&dt=0&PageNo=6\n",
      "Navigating to page URL...\n",
      "  > Scraping page 7: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=5&dt=0&PageNo=7\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2024.1 Men's Doubles.csv:\n",
      "Waiting for 16.44 seconds...\n",
      "Scraping data for: '2024.1 Beginners'...\n",
      "Navigating to URL...\n",
      "Found 4 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=7&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=7&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "  > Scraping page 3: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=7&dt=0&PageNo=3\n",
      "Navigating to page URL...\n",
      "  > Scraping page 4: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1251&EId=7&dt=0&PageNo=4\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2024.1 Beginners.csv:\n",
      "Waiting for 32.58 seconds...\n",
      "Scraping data for: '2024.2 Men's Singles Open'...\n",
      "Navigating to URL...\n",
      "Found 5 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=1&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=1&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "  > Scraping page 3: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=1&dt=0&PageNo=3\n",
      "Navigating to page URL...\n",
      "  > Scraping page 4: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=1&dt=0&PageNo=4\n",
      "Navigating to page URL...\n",
      "  > Scraping page 5: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=1&dt=0&PageNo=5\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2024.2 Men's Singles Open.csv:\n",
      "Waiting for 43.58 seconds...\n",
      "Scraping data for: '2024.2 Women's Doubles'...\n",
      "Navigating to URL...\n",
      "Found 10 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=2&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=2&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "  > Scraping page 3: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=2&dt=0&PageNo=3\n",
      "Navigating to page URL...\n",
      "  > Scraping page 4: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=2&dt=0&PageNo=4\n",
      "Navigating to page URL...\n",
      "  > Scraping page 5: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=2&dt=0&PageNo=5\n",
      "Navigating to page URL...\n",
      "  > Scraping page 6: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=2&dt=0&PageNo=6\n",
      "Navigating to page URL...\n",
      "  > Scraping page 7: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=2&dt=0&PageNo=7\n",
      "Navigating to page URL...\n",
      "  > Scraping page 8: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=2&dt=0&PageNo=8\n",
      "Navigating to page URL...\n",
      "  > Scraping page 9: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=2&dt=0&PageNo=9\n",
      "Navigating to page URL...\n",
      "  > Scraping page 10: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=2&dt=0&PageNo=10\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2024.2 Women's Doubles.csv:\n",
      "Waiting for 19.27 seconds...\n",
      "Scraping data for: '2024.2 Team Relay Men'...\n",
      "Navigating to URL...\n",
      "Found 2 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=5&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=5&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2024.2 Team Relay Men.csv:\n",
      "Waiting for 22.55 seconds...\n",
      "Scraping data for: '2024.2 Team Relay Mixed'...\n",
      "Navigating to URL...\n",
      "Found 2 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=6&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=6&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2024.2 Team Relay Mixed.csv:\n",
      "Waiting for 24.56 seconds...\n",
      "Scraping data for: '2024.2 Team Relay Women'...\n",
      "Navigating to URL...\n",
      "Found 2 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=4&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1252&EId=4&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2024.2 Team Relay Women.csv:\n",
      "Waiting for 31.18 seconds...\n",
      "Scraping data for: '2023.1 Women's Singles Open'...\n",
      "Navigating to URL...\n",
      "Found 2 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1216&EId=1&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1216&EId=1&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2023.1 Women's Singles Open.csv:\n",
      "Waiting for 37.85 seconds...\n",
      "Scraping data for: '2023.1 Women's Singles Competitive'...\n",
      "Navigating to URL...\n",
      "Found 2 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1216&EId=2&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1216&EId=2&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2023.1 Women's Singles Competitive.csv:\n",
      "Waiting for 41.36 seconds...\n",
      "Scraping data for: '2023.1 Men's Singles Open'...\n",
      "Navigating to URL...\n",
      "Found 2 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1216&EId=3&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1216&EId=3&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2023.1 Men's Singles Open.csv:\n",
      "Waiting for 31.84 seconds...\n",
      "Scraping data for: '2023.1 Men's Singles Competitive'...\n",
      "Navigating to URL...\n",
      "Found 3 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1216&EId=4&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1216&EId=4&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "  > Scraping page 3: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1216&EId=4&dt=0&PageNo=3\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2023.1 Men's Singles Competitive.csv:\n",
      "Waiting for 18.38 seconds...\n",
      "Scraping data for: '2023.2 Women's Doubles'...\n",
      "Navigating to URL...\n",
      "Found 3 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=1&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=1&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "  > Scraping page 3: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=1&dt=0&PageNo=3\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2023.2 Women's Doubles.csv:\n",
      "Waiting for 10.75 seconds...\n",
      "Scraping data for: '2023.2 Men's Doubles'...\n",
      "Navigating to URL...\n",
      "Found 3 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=2&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=2&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "  > Scraping page 3: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=2&dt=0&PageNo=3\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2023.2 Men's Doubles.csv:\n",
      "Waiting for 23.75 seconds...\n",
      "Scraping data for: '2023.2 Mixed Doubles'...\n",
      "Navigating to URL...\n",
      "Found 3 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=3&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=3&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "  > Scraping page 3: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=3&dt=0&PageNo=3\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2023.2 Mixed Doubles.csv:\n",
      "Waiting for 28.18 seconds...\n",
      "Scraping data for: '2023.2 Team Relay Women'...\n",
      "Navigating to URL...\n",
      "Found 1 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=4&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2023.2 Team Relay Women.csv:\n",
      "Waiting for 37.91 seconds...\n",
      "Scraping data for: '2023.2 Team Relay Men'...\n",
      "Navigating to URL...\n",
      "Found 1 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=5&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2023.2 Team Relay Men.csv:\n",
      "Waiting for 20.61 seconds...\n",
      "Scraping data for: '2023.2 Team Relay Mixed'...\n",
      "Navigating to URL...\n",
      "Found 2 page(s) for this event.\n",
      "  > Scraping page 1: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=6&dt=0&PageNo=1\n",
      "Navigating to page URL...\n",
      "  > Scraping page 2: https://runnersunite.racetecresults.com/results.aspx?CId=16634&RId=1217&EId=6&dt=0&PageNo=2\n",
      "Navigating to page URL...\n",
      "Writing file rl_data/2023.2 Team Relay Mixed.csv:\n",
      "Waiting for 18.16 seconds...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for each of the extracted data items\n",
    "for data in extracted_data:\n",
    "    #get the text and href\n",
    "    event_name = data['text']\n",
    "    url = data['href']\n",
    "    print(f\"Scraping data for: '{event_name}'...\")\n",
    "         \n",
    "    try:\n",
    "\n",
    "        # Setup and run a real Chrome browser\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "        print(\"Navigating to URL...\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the page (and any javascript) to load\n",
    "        time.sleep(3) \n",
    "\n",
    "        # Get the HTML content after the browser has rendered it\n",
    "        html_content = driver.page_source\n",
    "        \n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "        \n",
    "\n",
    "        total_pages = get_total_pages(html_content)\n",
    "        print(f\"Found {total_pages} page(s) for this event.\")\n",
    "\n",
    "        # set list of dataframes to empty\n",
    "        final_event_dfs = []\n",
    "\n",
    "        # --- Step 3-6: Loop through all pages ---\n",
    "        for page_num in range(1, total_pages + 1):\n",
    "            page_url = f\"{url}&dt=0&PageNo={page_num}\"\n",
    "            print(f\"  > Scraping page {page_num}: {page_url}\")\n",
    "            \n",
    "            # We can use pandas directly here for simplicity\n",
    "            try:\n",
    "\n",
    "                # Setup and run a real Chrome browser\n",
    "                driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "                print(\"Navigating to page URL...\")\n",
    "                driver.get(page_url)\n",
    "\n",
    "                # Wait for the page (and any javascript) to load\n",
    "                time.sleep(3) \n",
    "\n",
    "                # Get the HTML content after the browser has rendered it\n",
    "                html_content = driver.page_source\n",
    "\n",
    "                # Close the browser\n",
    "                driver.quit()\n",
    "\n",
    "                # 1. Isolate the table using BeautifulSoup\n",
    "                soup = BeautifulSoup(html_content, 'lxml')\n",
    "                results_div = soup.find('div', id='ctl00_Content_Main_divGrid')\n",
    "                if not results_div:\n",
    "                    raise ValueError(\"Could not find the main results container div.\")\n",
    "\n",
    "                results_table_html = str(results_div.find('table'))\n",
    "\n",
    "                # 2. Use pandas to parse the isolated HTML table\n",
    "                # read_html returns a list of DataFrames. We want the first one.\n",
    "                try:\n",
    "                    df = pd.read_html(StringIO(results_table_html))[0]\n",
    "                    \n",
    "                    #if this is not the first df\n",
    "                    if(final_event_dfs != []):\n",
    "                        #remove the first row of the df\n",
    "                        df = df.iloc[1:]\n",
    "                    \n",
    "                    #add to final event list of df\n",
    "                    final_event_dfs.append(df)\n",
    "        \n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"pandas.read_html failed to parse the table. Error: {e}\")\n",
    "\n",
    "        # This list will store the pandas DataFrame for each scraped event\n",
    "            except Exception as e:\n",
    "                print(f\"    - Could not scrape table from page {page_num}. Error: {e}\")\n",
    "\n",
    "        # --- Clean Up the DataFrame ---\n",
    "        #now we should have a list of dataframes\n",
    "        final_df = pd.concat(final_event_dfs, ignore_index=True)\n",
    "\n",
    "        #print(f\"Data Cleaning and Processing {event_name}\")\n",
    "               \n",
    "        # Step 1: Promote the first row of data to be the DataFrame's column headers\n",
    "        final_df.columns = final_df.iloc[0]\n",
    "\n",
    "        # Step 2: Remove the now-redundant first row (which is now just a copy of the headers)\n",
    "        # and reset the index to be clean (0, 1, 2...).\n",
    "        final_df = final_df.iloc[1:].reset_index(drop=True)\n",
    "        \n",
    "        # Step 3: Clean the newly assigned column headers.\n",
    "        # Use str(col).strip() to be safe. This prevents errors if a column name was NaN or another non-string.\n",
    "        final_df.columns = [str(col).strip() for col in final_df.columns]\n",
    "        final_df.columns.name = None # Clean up the index name left over from the promotion\n",
    "\n",
    "        #Remove Multiple instances of the same column, mobile / desktop\n",
    "        final_df = remove_second_duplicate_column(final_df, \"Name\")\n",
    "        final_df = remove_second_duplicate_column(final_df, \"Net Time\")\n",
    "                  \n",
    "        # write the master df to cvs\n",
    "        filename = f\"rl_data/{event_name}.csv\"\n",
    "        print(f\"Writing file {filename}:\")\n",
    "        final_df.to_csv(filename, index=False) \n",
    "             \n",
    "    except Exception as e:\n",
    "        print(f\"--> An error occurred while scraping {url}: {e}\")\n",
    "\n",
    "    # Delay between requests so as not to be too annoying \n",
    "    polite_delay()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Finalising the files.\n",
    "Just to some simple renaming of files, so they match my Redline fitness games Explorerer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming rl_data/2024.1 Women's Singles Competitive.csv to rl_data/WomensSinglesCompetitive2024.csv\n",
      "Renaming rl_data/2024.1 Men's Singles Competitive.csv to rl_data/MensSinglesCompetitive2024.csv\n",
      "Renaming rl_data/2024.1 Women's Singles Open.csv to rl_data/WomensSinglesOpen2024.csv\n",
      "Renaming rl_data/2024.2 Men's Singles Open.csv to rl_data/MensSinglesOpen2024.csv\n",
      "Renaming rl_data/2024.2 Women's Doubles.csv to rl_data/WomensDoubles2024.csv\n",
      "Renaming rl_data/2024.1 Men's Doubles.csv to rl_data/MensDoubles2024.csv\n",
      "Renaming rl_data/2024.1 Mixed Doubles.csv to rl_data/MixedDoubles2024.csv\n",
      "Renaming rl_data/2024.2 Team Relay Women.csv to rl_data/TeamRelayWomen2024.csv\n",
      "Renaming rl_data/2024.2 Team Relay Men.csv to rl_data/TeamRelayMen2024.csv\n",
      "Renaming rl_data/2024.2 Team Relay Mixed.csv to rl_data/TeamRelayMixed2024.csv\n",
      "Renaming rl_data/2023.1 Women's Singles Competitive.csv to rl_data/WomensSinglesCompetitive2023.csv\n",
      "Renaming rl_data/2023.1 Men's Singles Competitive.csv to rl_data/MensSinglesCompetitive2023.csv\n",
      "Renaming rl_data/2023.1 Women's Singles Open.csv to rl_data/WomensSinglesOpen2023.csv\n",
      "Renaming rl_data/2023.1 Men's Singles Open.csv to rl_data/MensSinglesOpen2023.csv\n",
      "Renaming rl_data/2023.2 Women's Doubles.csv to rl_data/WomensDoubles2023.csv\n",
      "Renaming rl_data/2023.2 Men's Doubles.csv to rl_data/MensDoubles2023.csv\n",
      "Renaming rl_data/2023.2 Mixed Doubles.csv to rl_data/MixedDoubles2023.csv\n",
      "Renaming rl_data/2023.2 Team Relay Women.csv to rl_data/TeamRelayWomen2023.csv\n",
      "Renaming rl_data/2023.2 Team Relay Men.csv to rl_data/TeamRelayMen2023.csv\n",
      "Renaming rl_data/2023.2 Team Relay Mixed.csv to rl_data/TeamRelayMixed2023.csv\n"
     ]
    }
   ],
   "source": [
    "#rename the files to match Redline Explorer expected format\n",
    "\n",
    "for i in range(len(EVENT_RENAME_LIST)):\n",
    "    original_file_name = f\"rl_data/{EVENT_RENAME_LIST[i][1]}.csv\"\n",
    "    new_file_name = f\"rl_data/{EVENT_RENAME_LIST[i][0]}.csv\"\n",
    "    print(f\"Renaming '{original_file_name}' to '{new_file_name}'\")\n",
    "    os.rename(original_file_name, new_file_name)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
